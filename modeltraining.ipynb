{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import joblib\n",
    "from tkinter import * \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve \n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import learning_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(y_test, y_pred,modelname):\n",
    "    '''\n",
    "    This function read the true outcome and the predicted outcome,\n",
    "    and draw a confusion matrix of specific model\n",
    "\n",
    "    **Parameters**\n",
    "        y_test: *array*\n",
    "            The real outcome\n",
    "        y_pred: *array*\n",
    "            The predicted outcome\n",
    "        modelname: *str*\n",
    "            The name of model\n",
    "\n",
    "    **Return**\n",
    "        None\n",
    "    '''\n",
    "    # calculate the confusion matrix based on different model \n",
    "    conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "    # plot the confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    ax.matshow(conf_matrix, cmap='GnBu', alpha=0.75)\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='large') \n",
    "    plt.xlabel('Predictions', fontsize=10)\n",
    "    plt.ylabel('Actuals', fontsize=10)\n",
    "    plt.title('Confusion Matrix: %s '% modelname, fontsize=12)\n",
    "    fig.savefig('Figure/%s_confusion_matrix.png' % modelname)\n",
    "\n",
    "\n",
    "def ROC_curve(y_test, y_pred_proba,modelname):\n",
    "    '''\n",
    "    This function read the true outcome and the probablity of the positive predicted outcomes,\n",
    "    and draw a ROC curve of specific model\n",
    "\n",
    "    **Parameters**\n",
    "        y_test: *array*\n",
    "            The real outcome\n",
    "        y_pred_proba: *array*\n",
    "            The probablity of the positive predicted outcomes\n",
    "        modelname: *str*\n",
    "            The name of model\n",
    "\n",
    "    **Return**\n",
    "        None\n",
    "    '''\n",
    "    # calculate the false positive rate and true positive rate of model\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:,1])\n",
    "    # calculate the auc value of model\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig = plt.figure()\n",
    "    plt.plot(fpr, tpr, 'b', label = '%s (AUC = %0.2f)' % (modelname,roc_auc))\n",
    "    #plot the ROC curve and calculate AUC\n",
    "    plt.plot([0, 1], [0, 1],'r--', label='No Skill Classifier')\n",
    "    #plot the 'No Skill Classifier' curve\n",
    "    plt.plot([0, 0, 1], [0, 1, 1], linestyle=':', color='black', label='perfect performance')\n",
    "    #Plot the 'perfect performance' curve\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.title('ROC Curve: %s '% modelname)\n",
    "    plt.ylabel('True Positive Rate (sensitivity)')\n",
    "    plt.xlabel('False Positive Rate (1-specificity)')\n",
    "    fig.savefig(\"Figure/%s_ROC_curve.png\" % modelname)\n",
    "    \n",
    "\n",
    "def PR_curve(y_test, y_pred_proba,modelname):\n",
    "    '''\n",
    "    This function read the true outcome and the probablity of the positive predicted outcomes,\n",
    "    and draw a ROC curve of specific model\n",
    "\n",
    "    **Parameters**\n",
    "        y_test: *array*\n",
    "            The real outcome\n",
    "        y_pred_proba: *array*\n",
    "            The probablity of the positive predicted outcomes\n",
    "        modelname: *str*\n",
    "            The name of model\n",
    "\n",
    "    **Return**\n",
    "        None\n",
    "    '''\n",
    "    precision, recall, _= precision_recall_curve(y_test,y_pred_proba[:,1])\n",
    "    #calculate precision and recall\n",
    "    no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "    fig = plt.figure()\n",
    "    plt.plot(recall, precision, marker='.', label='%s ' % modelname)\n",
    "    #plot PR curve\n",
    "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill Classifier')\n",
    "    #plot 'No Skill Classifier' curve\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('PR Curve: %s '% modelname)\n",
    "    plt.legend()\n",
    "    fig.savefig(\"Figure/%s_PR_curve.png\" % modelname)\n",
    "\n",
    "\n",
    "def Scores(y_test,y_pred,y_pred_proba,modelname):\n",
    "    '''\n",
    "    This function read the true outcome, the predicted outcome \n",
    "    and the probablity of the positive predicted outcomes. \n",
    "    Then, it give back the precison, recall, F1 score and the ROC-AUC score of the model.\n",
    "\n",
    "    **Parameters**\n",
    "        y_test: *array*\n",
    "            The real outcome\n",
    "        y_pred: *array*\n",
    "            The predicted outcomes\n",
    "        y_pred_proba: *array*\n",
    "            The probablity of the positive predicted outcomes\n",
    "        modelname: *str*\n",
    "            The name of model\n",
    "\n",
    "    **Return**\n",
    "        None\n",
    "    '''\n",
    "    print('----------------------------')\n",
    "    print('This is %s'% modelname)\n",
    "    print('Precision: %.3f' % precision_score(y_test, y_pred))\n",
    "    print('Recall: %.3f' % recall_score(y_test, y_pred))\n",
    "    print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n",
    "    print('F1 Score: %.3f' % f1_score(y_test, y_pred))\n",
    "    #Calculate the scores by test set and probability estimates provided by the predict_pred\n",
    "    print('ROC-AUC Score: %.3f' % roc_auc_score(y_test, y_pred_proba[:,1]))\n",
    "    #Calculate the ROC-AUC score by test set result probability estimates provided by the predict_proba\n",
    "    return None\n",
    "    \n",
    "\n",
    "def plot_learning_curve(dataset, estimator,modelname):\n",
    "    '''\n",
    "    This function read the dataset and give back its learning curve based on specific model.\n",
    "\n",
    "    **Parameters**\n",
    "        dataset: *dataframe*\n",
    "            The dataframe used for model fitting\n",
    "        estimator: *array*\n",
    "            The fitting model\n",
    "        modelname: *str*\n",
    "            The name of model\n",
    "\n",
    "    **Return**\n",
    "        None\n",
    "    '''\n",
    "    x=dataset.drop(columns='HeartDisease')\n",
    "    #Dataset except target\n",
    "    y=dataset['HeartDisease']\n",
    "    #Dataset of target\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, x, y)\n",
    "    #Number of samples in training set, score if training set, score of test set\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    #Mean of test set scores\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    #Standard deviation of test scores\n",
    "    fig = plt.figure()\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title('Learning Curve: %s '% modelname, fontsize='small')\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                         alpha=0.1, color=\"b\") \n",
    "    #Plot the learning curve with upper and lower limits of training score\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                         alpha=0.1, color=\"r\")\n",
    "    #Plot the learning curve with upper and lower limits of test score\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n",
    "    #Plot learning curve\n",
    "    plt.legend(loc=\"best\")\n",
    "    fig.savefig(\"Figure/%s_Learning_curve.png\" % modelname)\n",
    "\n",
    "\n",
    "def RandomForest(dataset):\n",
    "    '''\n",
    "    This function read a dataset and train it on random forest model.\n",
    "\n",
    "    **Parameters**\n",
    "        dataset: *dataframe*\n",
    "            The addressed dataframe\n",
    "\n",
    "    **Return**\n",
    "        None\n",
    "    '''\n",
    "    X = dataset.drop(['Category'], axis=1)\n",
    "    #Dataset except target\n",
    "    Y = dataset['Category']\n",
    "    #Dataset of target\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=100)\n",
    "    #Split data to training set and test set\n",
    "    transfer = StandardScaler()\n",
    "    x_train = transfer.fit_transform(x_train)\n",
    "    x_test = transfer.fit_transform(x_test)\n",
    "    #Feature scaling\n",
    "    rf = RandomForestClassifier(n_estimators=10, n_jobs=2)\n",
    "    rf.fit(x_train,y_train)\n",
    "    #Fit the model to training set\n",
    "    y_pred=rf.predict(x_test)\n",
    "    #Predict the result of test set\n",
    "    y_pred_proba=rf.predict_proba(x_test)\n",
    "    #Predict the result of test set to plot ROC and calculate AUC\n",
    "    #joblib.dump(rf, \"Model/rf_model.joblib\" ,compress=1)\n",
    "    plot_confusion_matrix(y_test,y_pred,'RandomForest')\n",
    "    ROC_curve(y_test, y_pred_proba,'RandomForest')\n",
    "    PR_curve(y_test, y_pred_proba,'RandomForest')\n",
    "    plot_learning_curve(dataset,rf,'RandomForest')\n",
    "    Scores(y_test,y_pred,y_pred_proba,'RandomForest')\n",
    "    return y_test, y_pred, y_pred_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocesseddata.csv').drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "RandomForest(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Multi class logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "le = LabelEncoder()\n",
    "sc = StandardScaler()\n",
    "\n",
    "X = pd.get_dummies(df.iloc[:, 1:], drop_first=True).values.astype('float')\n",
    "y = le.fit_transform(df.iloc[:, 0].values).astype('float')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
